{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f33bc2",
   "metadata": {},
   "source": [
    "# Question 7 – Decision Tree and Random Forest on Pima Diabetes\n",
    "Dataset: `pima-diabetes.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3996d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome value counts (after mapping):\n",
      "Outcome\n",
      "0    500\n",
      "1    268\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def load_pima():\n",
    "    for p in [\n",
    "        'pima-diabetes.xlsx', 'pima_diabetes.xlsx',\n",
    "        '/mnt/data/pima-diabetes.xlsx', '/mnt/data/pima_diabetes.xlsx'\n",
    "    ]:\n",
    "        if os.path.exists(p):\n",
    "            path = p\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError('pima-diabetes.xlsx not found')\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    # Normalize Outcome name and map strings to 0/1\n",
    "    outcome_cols = [c for c in df.columns if c.strip().lower() == 'outcome']\n",
    "    if outcome_cols:\n",
    "        oc = outcome_cols[0]\n",
    "        if oc != 'Outcome':\n",
    "            df = df.rename(columns={oc: 'Outcome'})\n",
    "\n",
    "    df['Outcome'] = (\n",
    "        df['Outcome']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .map({'Non-Diabetic': 0, 'Diabetic': 1})\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "pima = load_pima()\n",
    "print(\"Outcome value counts (after mapping):\")\n",
    "print(pima['Outcome'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c556a",
   "metadata": {},
   "source": [
    "## 7(a) Question – Tunable Parameters and Their Effects\n",
    "> When using decision tree and random forest to predict diabetes, what parameters can be tuned for\n",
    "> each model? What are the effects of increasing or decreasing these parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853ef532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For modeling: drop rows with missing Outcome only\n",
    "model_df = pima.dropna(subset=['Outcome']).copy()\n",
    "\n",
    "X = model_df.drop(columns=['Outcome'])\n",
    "y = model_df['Outcome']\n",
    "\n",
    "# Column-wise median imputation for features (no list deletion)\n",
    "X_imputed = X.copy()\n",
    "for col in X_imputed.columns:\n",
    "    med = X_imputed[col].median(skipna=True)\n",
    "    X_imputed[col] = X_imputed[col].fillna(med)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y,\n",
    "    test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA features\n",
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cb3c3",
   "metadata": {},
   "source": [
    "### 7(a) Explanation – Hyperparameters\n",
    "\n",
    "**Decision Tree:**\n",
    "- `max_depth`: maximum depth of the tree; increasing depth → more complex model, risk of overfitting.\n",
    "- `min_samples_split`: minimum number of samples to split; larger → fewer splits (simpler tree).\n",
    "- `min_samples_leaf`: minimum samples in a leaf; larger → smoother decision boundaries.\n",
    "- `criterion`: `gini` or `entropy` for impurity measure.\n",
    "\n",
    "**Random Forest:**\n",
    "- `n_estimators` ($k$ in slides): number of trees. Larger $k$ reduces variance but increases computation.\n",
    "- `max_depth`, `min_samples_split`, `min_samples_leaf`: control tree complexity.\n",
    "- `max_features` ($d$): number of features considered at each split. Smaller $d$ increases randomness and diversity\n",
    "  among trees, often improving generalization (slides suggest $d \\approx \\sqrt{m}$ for classification).\n",
    "\n",
    "Tuning these parameters balances **bias vs variance** and controls overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79aef85",
   "metadata": {},
   "source": [
    "## 7(b) Question – Overfitting and Detection\n",
    "> What is overfitting? How do you check whether overfitting happens or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc4f47",
   "metadata": {},
   "source": [
    "### 7(b) Explanation – Overfitting\n",
    "\n",
    "Overfitting occurs when a model learns **noise** and specific details in the training data that do not generalize.\n",
    "Symptoms:\n",
    "- **Very high training accuracy**.\n",
    "- **Lower test accuracy** (large train–test performance gap).\n",
    "\n",
    "To detect overfitting:\n",
    "1. Split data into training and testing sets.\n",
    "2. Train the model on training set.\n",
    "3. Evaluate accuracy (or other metrics) on both sets.\n",
    "4. If training >> test, overfitting is likely.\n",
    "\n",
    "Decision trees with high depth are prone to overfitting; random forests reduce this by averaging many trees built on\n",
    "bootstrap samples and random feature subsets (bagging & random subspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa064bed",
   "metadata": {},
   "source": [
    "## 7(c) Question – Predictions with Full Features vs PCA Features\n",
    "> Make a prediction of Outcome using:\n",
    "> 1. All original features.\n",
    "> 2. Features obtained from PCA analysis.\n",
    "> Compare the results and state your conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01883402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (full) train acc: 0.8491620111731844\n",
      "Decision Tree (full) test acc: 0.7316017316017316\n",
      "Random Forest (full) train acc: 1.0\n",
      "Random Forest (full) test acc: 0.7445887445887446\n",
      "Decision Tree (PCA) train acc: 0.8361266294227188\n",
      "Decision Tree (PCA) test acc: 0.7229437229437229\n",
      "Random Forest (PCA) train acc: 1.0\n",
      "Random Forest (PCA) test acc: 0.7359307359307359\n",
      "\n",
      "Classification report for Random Forest (full features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       150\n",
      "           1       0.67      0.53      0.59        81\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.70      0.70       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7(c) – Train decision tree and random forest with full features and PCA features\n",
    "\n",
    "# Decision tree with original features\n",
    "tree_full = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_full.fit(X_train, y_train)\n",
    "y_pred_tree_full_train = tree_full.predict(X_train)\n",
    "y_pred_tree_full_test = tree_full.predict(X_test)\n",
    "acc_tree_full_train = accuracy_score(y_train, y_pred_tree_full_train)\n",
    "acc_tree_full_test = accuracy_score(y_test, y_pred_tree_full_test)\n",
    "print(\"Decision Tree (full) train acc:\", acc_tree_full_train)\n",
    "print(\"Decision Tree (full) test acc:\", acc_tree_full_test)\n",
    "\n",
    "# Random forest with original features\n",
    "rf_full = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)\n",
    "rf_full.fit(X_train, y_train)\n",
    "y_pred_rf_full_train = rf_full.predict(X_train)\n",
    "y_pred_rf_full_test = rf_full.predict(X_test)\n",
    "acc_rf_full_train = accuracy_score(y_train, y_pred_rf_full_train)\n",
    "acc_rf_full_test = accuracy_score(y_test, y_pred_rf_full_test)\n",
    "print(\"Random Forest (full) train acc:\", acc_rf_full_train)\n",
    "print(\"Random Forest (full) test acc:\", acc_rf_full_test)\n",
    "\n",
    "# Decision tree with PCA features\n",
    "tree_pca = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_pca.fit(X_train_pca, y_train)\n",
    "y_pred_tree_pca_train = tree_pca.predict(X_train_pca)\n",
    "y_pred_tree_pca_test = tree_pca.predict(X_test_pca)\n",
    "acc_tree_pca_train = accuracy_score(y_train, y_pred_tree_pca_train)\n",
    "acc_tree_pca_test = accuracy_score(y_test, y_pred_tree_pca_test)\n",
    "print(\"Decision Tree (PCA) train acc:\", acc_tree_pca_train)\n",
    "print(\"Decision Tree (PCA) test acc:\", acc_tree_pca_test)\n",
    "\n",
    "# Random forest with PCA features\n",
    "rf_pca = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)\n",
    "rf_pca.fit(X_train_pca, y_train)\n",
    "y_pred_rf_pca_train = rf_pca.predict(X_train_pca)\n",
    "y_pred_rf_pca_test = rf_pca.predict(X_test_pca)\n",
    "acc_rf_pca_train = accuracy_score(y_train, y_pred_rf_pca_train)\n",
    "acc_rf_pca_test = accuracy_score(y_test, y_pred_rf_pca_test)\n",
    "print(\"Random Forest (PCA) train acc:\", acc_rf_pca_train)\n",
    "print(\"Random Forest (PCA) test acc:\", acc_rf_pca_test)\n",
    "\n",
    "print(\"\\nClassification report for Random Forest (full features):\")\n",
    "print(classification_report(y_test, y_pred_rf_full_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82823a2e",
   "metadata": {},
   "source": [
    "### 7(c) Explanation – Full vs PCA Features\n",
    "\n",
    "**Analysis**\n",
    "**1. Decision Tree (full features)**\n",
    "- Train accuracy = **0.849**  \n",
    "- Test accuracy = **0.732**  \n",
    "- Shows moderate overfitting.  \n",
    "- Sensitive to noise because a single tree forms very specific rules.\n",
    "\n",
    "**2. Random Forest (full features)**\n",
    "- Train accuracy = **1.000** → Expected overfitting (forest memorizes training data).  \n",
    "- Test accuracy = **0.745**, the highest among all models.  \n",
    "- Combines multiple trees → better generalization.\n",
    "\n",
    "**3. Effect of PCA on Tree Models**\n",
    "- PCA compresses and mixes original features → decision trees lose interpretability of splits.  \n",
    "- Both tree and forest with PCA have **slightly lower** test accuracy:\n",
    "  - DT PCA = 0.723  \n",
    "  - RF PCA = 0.736  \n",
    "- PCA is not ideal for tree models, which prefer axis-aligned splits on original variables.\n",
    "\n",
    "**Conclusion:**  \n",
    "**Random Forest with full features performs the best** and generalizes better than all PCA-based models.\n",
    "\n",
    "**Interpretation**\n",
    "- Class **0** (non-diabetic) is much easier to classify due to being the majority class.  \n",
    "- Class **1** has lower recall (0.53) → many diabetics are missed.  \n",
    "- This confirms the dataset suffers from **class imbalance**.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
